# Kubernetes Cluster Automation on OpenStack

**Automated deployment of Kubernetes clusters on existing OpenStack cloud infrastructure using Terraform, Ansible, and kubeadm.**

Deploy a 3-node Kubernetes cluster (1 master + 2 workers) with Cilium CNI in ~15 minutes!

---

## âš ï¸ Important Notice

**This automation is designed for demonstration and learning purposes.**

- âœ… **Best for:** Labs, demos, development, learning Kubernetes and OpenStack
- âœ… **Use cases:** Testing applications, experimenting with K8s features, training environments
- âŒ **NOT recommended for production:** Lacks HA, advanced security hardening, and production-grade monitoring
- ğŸ“š **Deployment method:** kubeadm (standard Kubernetes deployment tool)

---

## ğŸ“‹ Prerequisites

### 1. OpenStack Cloud (Required)

You must have an **existing OpenStack cloud** that is already installed and operational. This automation deploys Kubernetes **on top of** OpenStack - it does NOT install OpenStack.

**Tested with:**
- OpenStack: **2025.1 (Epoxy)** via [OpenStack-Ansible AIO](https://docs.openstack.org/openstack-ansible/2025.1/user/aio/quickstart.html)
- Kubernetes: **v1.33.5**
- Cilium CNI: **v1.16.4**

### 2. OpenStack Environment Setup (CRITICAL FIRST STEP)

**Before running the automation**, you MUST prepare your OpenStack environment:

```bash
# 1. Copy the setup script to your OpenStack control plane host
scp scripts/setup-openstack-environment.sh user@openstack-host:~/

# 2. SSH to OpenStack host and run the script
ssh user@openstack-host
./setup-openstack-environment.sh
```

**What this script does:**
- Creates OpenStack domain, project, and user
- Sets up networking (external network with floating IPs)
- Uploads Ubuntu 24.04 cloud image
- Creates VM flavor
- **Fixes networking issues** (applies NAT rules for VM connectivity)
- Generates OpenStack credentials file

**âš ï¸ Important:** This step is mandatory! Without it, VMs won't have proper network connectivity.

### 3. Local Machine Requirements

- **Terraform** >= 0.14.0
- **Ansible** >= 2.9
- **OpenStack CLI** configured and authenticated
- **SSH access** to OpenStack control plane
- **Network connectivity** to OpenStack floating IP range

---

## ğŸš€ Quick Start

```bash
# 1. Source OpenStack credentials (generated by setup script)
source ~/openrc-learning-admin

# 2. Run the complete deployment
./deploy-complete.sh
```

**Total deployment time: ~15 minutes**

---

## ğŸŒ Network Architecture

### Network Subnets Overview

The automation creates and uses multiple network layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenStack External Network (Public)                             â”‚
â”‚ 172.29.248.0/22                                                  â”‚
â”‚ â”œâ”€ Gateway: 172.29.248.1                                         â”‚
â”‚ â””â”€ Floating IP Pool: 172.29.249.100 - 172.29.249.200            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ NAT (OpenStack Router)
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kubernetes Cluster Network (Private)                            â”‚
â”‚ 192.168.100.0/24                                                 â”‚
â”‚ â”œâ”€ k8s-cluster-master:    192.168.100.x â†’ 172.29.249.xxx        â”‚
â”‚ â”œâ”€ k8s-cluster-worker-1:  192.168.100.x â†’ 172.29.249.xxx        â”‚
â”‚ â””â”€ k8s-cluster-worker-2:  192.168.100.x â†’ 172.29.249.xxx        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Kubernetes Networking
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kubernetes Service Network (Virtual)                            â”‚
â”‚ 10.96.0.0/12                                                     â”‚
â”‚ â””â”€ ClusterIP services, DNS, etc.                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Cilium CNI
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kubernetes Pod Network (Virtual)                                â”‚
â”‚ 10.244.0.0/16                                                    â”‚
â”‚ â””â”€ Individual pod IPs across all nodes                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Network Components Breakdown

| Network Layer | CIDR | Purpose | Managed By |
|---------------|------|---------|------------|
| **OpenStack External** | `172.29.248.0/22` | Floating IPs for external access to VMs | OpenStack Neutron |
| **K8s Cluster Private** | `192.168.100.0/24` | VM-to-VM communication (private network) | Terraform |
| **K8s Services** | `10.96.0.0/12` | ClusterIP services, kube-dns | kubeadm |
| **K8s Pods** | `10.244.0.0/16` | Pod-to-pod communication | Cilium CNI |

### Traffic Flow Examples

1. **External â†’ Master Node:**
   ```
   Internet/Local â†’ 172.29.249.xxx (Floating IP) â†’ 192.168.100.x (Master Private IP)
   ```

2. **Pod-to-Pod Communication:**
   ```
   Pod 10.244.1.5 â†’ Cilium VXLAN â†’ Pod 10.244.2.8
   ```

3. **Pod-to-Service:**
   ```
   Pod 10.244.1.5 â†’ Service 10.96.0.10:80 â†’ Backend Pods
   ```

4. **Node-to-Node:**
   ```
   192.168.100.56 (Master) â†” 192.168.100.58 (Worker) via OpenStack private network
   ```

### Port Requirements

All required ports are automatically configured in OpenStack security groups:

- **22** - SSH access
- **6443** - Kubernetes API server
- **2379-2380** - etcd
- **10250** - kubelet API
- **10259** - kube-scheduler
- **10257** - kube-controller-manager
- **30000-32767** - NodePort services
- **4240** - Cilium health checks
- **8472** - Cilium VXLAN (pod networking)

---

# Automation Workflow Documentation

## 1. AUTOMATION ENTRY POINT

### Main Entry Point: `deploy-complete.sh`

**Single command deployment:**
```bash
./deploy-complete.sh
```

This script orchestrates the entire deployment in 3 sequential steps:
- Automatically sources OpenStack credentials if needed
- Executes Step A (Infrastructure)
- Executes Step B (Kubernetes Init)
- Executes Step C (Workers Join)
- Displays cluster information and saves deployment details

**Total deployment time: 10-15 minutes**

---

## 2. DETAILED EXECUTION FLOW

### ğŸ“Š High-Level Flow

```
deploy-complete.sh
    â†“
    â”œâ”€â†’ Step A: step-a-provision-infrastructure.yml
    â”‚       â†“
    â”‚       â””â”€â†’ scripts/terraform-ops.sh (init/plan/apply)
    â”‚               â†“
    â”‚               â””â”€â†’ Terraform files:
    â”‚                   â”œâ”€ provider.tf
    â”‚                   â”œâ”€ variables.tf
    â”‚                   â”œâ”€ infrastructure.tf (MERGED: network+security+compute)
    â”‚                   â”œâ”€ outputs.tf
    â”‚                   â”œâ”€ cloud-init.yaml
    â”‚                   â””â”€ inventory.tpl
    â”‚                       â†“
    â”‚                       Generates: inventory-dynamic.yml & k8s-cluster-key.pem
    â†“
    â”œâ”€â†’ Step B: step-b-kubernetes-init.yml
    â”‚       â†“
    â”‚       â”œâ”€â†’ Phase 1: Common Setup (all nodes)
    â”‚       â”‚   - Disable swap + reboot
    â”‚       â”‚   - Load kernel modules
    â”‚       â”‚   - Install containerd
    â”‚       â”‚   - Install Kubernetes components
    â”‚       â”‚   - Configure /etc/hosts
    â”‚       â†“
    â”‚       â”œâ”€â†’ Phase 2: playbooks/master-init.yml
    â”‚       â”‚   - Generate kubeadm config from template
    â”‚       â”‚   - Run kubeadm init
    â”‚       â”‚   - Setup kubeconfig
    â”‚       â”‚   - Generate join command
    â”‚       â†“
    â”‚       â””â”€â†’ Phase 3: playbooks/install-cilium.yml
    â”‚           - Install Cilium CLI
    â”‚           - Deploy Cilium v1.16.4
    â”‚           - Wait for Cilium ready
    â†“
    â””â”€â†’ Step C: step-c-workers-join.yml
            â†“
            â”œâ”€â†’ Phase 1: playbooks/workers-join.yml
            â”‚   - Retrieve join command from master
            â”‚   - Join workers to cluster
            â†“
            â”œâ”€â†’ Phase 2: Wait for cluster readiness
            â”‚   - All nodes Ready (10m timeout)
            â”‚   - Cilium DaemonSet rollout (10m timeout)
            â†“
            â””â”€â†’ Phase 3: playbooks/verify-cluster.yml
                - Verify nodes Ready
                - Verify pods Running
                - Check Cilium status
                - Display summary
```

---

## 3. FILE-BY-FILE EXPLANATION (Execution Order)

### ğŸ¯ Entry Point

#### `deploy-complete.sh`
**Purpose:** Main orchestration script  
**What it does:**
- Checks for OpenStack credentials
- Calls 3 step playbooks sequentially
- Displays cluster information
- Saves deployment info to file

**Exit on error:** Yes (set -e)  
**Calls:** step-a, step-b, step-c playbooks

---

### ğŸ“¦ STEP A: Infrastructure (Terraform)

#### `step-a-provision-infrastructure.yml`
**Purpose:** Ansible wrapper for Terraform operations  
**What it does:**
1. Validates OpenStack authentication
2. Calls `terraform-ops.sh init`
3. Calls `terraform-ops.sh plan`
4. Pauses for confirmation
5. Calls `terraform-ops.sh apply`
6. Verifies inventory and SSH key generated
7. Tests SSH connectivity (non-blocking)
8. Checks/adds route to floating IP network

**Dependencies:** OpenStack credentials sourced  
**Generates:** inventory-dynamic.yml, config/k8s-cluster-key.pem

---

#### `scripts/terraform-ops.sh`
**Purpose:** Terraform operations wrapper  
**What it does:**
- `init`: Initialize Terraform
- `plan`: Show execution plan
- `apply`: Apply configuration with auto-approve
- `destroy`: Destroy infrastructure with confirmation
- `show`: Display current state
- `output`: Display outputs
- `wait`: Wait for VMs and cloud-init completion

**Key feature:** Validates OpenStack auth before each operation

---

#### Terraform Files (executed by terraform-ops.sh):

**`terraform/provider.tf`**
- Configures OpenStack provider
- Sets `insecure = true` for self-signed certs
- Uses `endpoint_type = "public"`
- Reads credentials from environment variables

**`terraform/variables.tf`**
- Defines all configurable parameters:
  - `cluster_name` = "k8s-cluster"
  - `image_name` = "ubuntu-noble"
  - `flavor_master` = "my-large" (2 vCPU, 4GB RAM, 8GB disk)
  - `flavor_worker` = "my-large"
  - `worker_count` = 2
  - `network_cidr` = "192.168.100.0/24"
  - `public_network_name` = "public"

**`terraform/infrastructure.tf`** (MERGED FILE)
- **Network Resources:**
  - `openstack_networking_network_v2.k8s_network` - Private network
  - `openstack_networking_subnet_v2.k8s_subnet` - Subnet with DNS
  - `openstack_networking_router_v2.k8s_router` - Router
  - `openstack_networking_router_interface_v2` - Router interface

- **Security Resources:**
  - `openstack_compute_keypair_v2.k8s_keypair` - SSH keypair
  - `local_file.private_key` - Save private key to config/
  - `openstack_networking_secgroup_v2.k8s_secgroup` - Security group
  - 13x `openstack_networking_secgroup_rule_v2.*` - Security rules:
    - SSH (22)
    - ICMP (ping)
    - K8s API (6443)
    - etcd (2379-2380)
    - Kubelet (10250)
    - kube-scheduler (10259)
    - kube-controller-manager (10257)
    - NodePort services (30000-32767)
    - Cilium health (4240)
    - Cilium VXLAN (8472)
    - Internal cluster traffic (all)

- **Compute Resources:**
  - `openstack_compute_instance_v2.k8s_master` - Master VM
  - `openstack_compute_instance_v2.k8s_workers[2]` - Worker VMs (count=2)
  - `openstack_networking_floatingip_v2.master_fip` - Master floating IP
  - `openstack_networking_floatingip_v2.worker_fips[2]` - Worker floating IPs
  - `openstack_compute_floatingip_associate_v2.*` - FIP associations

**`terraform/outputs.tf`**
- Outputs all IPs (public/private)
- Outputs SSH key path
- **Generates dynamic inventory:** `local_file.ansible_inventory`
  - Uses `inventory.tpl` template
  - Creates `inventory-dynamic.yml` in project root
  - Uses `abspath()` for SSH key path

**`terraform/cloud-init.yaml`**
- Basic VM initialization
- Updates packages
- Installs basic utilities (curl, wget, git, unzip, htop)
- Creates ubuntu user with sudo access
- Disables UFW firewall

**`terraform/inventory.tpl`**
- Jinja2 template for Ansible inventory
- Dynamic host generation based on worker count
- Includes SSH settings and private IPs

**Total resources created: 28**

---

### âš™ï¸ STEP B: Kubernetes Initialization

#### `step-b-kubernetes-init.yml`
**Purpose:** Initialize Kubernetes on all nodes  
**What it does:**

**Phase 1: Common Setup (runs on all nodes)**
- Disable swap:
  - `swapoff -a`
  - Remove swap from fstab
  - Delete swap file
  - **Reboot system**
  
- Load kernel modules:
  - overlay
  - br_netfilter
  - Create persistent config in /etc/modules-load.d/
  
- Configure sysctl:
  - net.bridge.bridge-nf-call-iptables = 1
  - net.ipv4.ip_forward = 1
  - Apply settings

- Install containerd:
  - Add Docker GPG key
  - Add Docker repository
  - Install containerd.io package
  - Configure with SystemdCgroup = true
  - Restart containerd

- Install Kubernetes:
  - Add Kubernetes apt key
  - Add Kubernetes repository (v1.33)
  - Install kubeadm, kubelet, kubectl
  - Hold packages from auto-update
  - Enable kubelet service

- Update /etc/hosts:
  - Add all cluster nodes

**Phase 2: Import playbooks/master-init.yml**

**Phase 3: Import playbooks/install-cilium.yml**

**Phase 4: Final checks**
- Wait for API server readiness
- Display node status

**Duration:** ~5-8 minutes (includes reboot)

---

#### `playbooks/master-init.yml`
**Purpose:** Initialize Kubernetes master node  
**What it does:**
1. Generate kubeadm config from `config/kubeadm-config.yaml.j2` template
2. Run `kubeadm init --config /tmp/kubeadm-config.yaml`
3. Create .kube directory for ubuntu user
4. Copy admin.conf to user's kubeconfig
5. Generate join command: `kubeadm token create --print-join-command`
6. Save join command to `config/k8s-join-command`

**Template used:** `config/kubeadm-config.yaml.j2`
- Sets advertiseAddress to private_ip
- Kubernetes version: v1.33.0
- Service subnet: 10.96.0.0/12
- Pod subnet: 10.244.0.0/16
- DNS domain: cluster.local
- CRI socket: unix:///var/run/containerd/containerd.sock
- Cgroup driver: systemd

**Generates:** /etc/kubernetes/admin.conf, join command

---

#### `playbooks/install-cilium.yml`
**Purpose:** Install Cilium CNI  
**What it does:**
1. Check if Cilium CLI already installed
2. Download Cilium CLI from GitHub (latest)
3. Extract and install to /usr/local/bin/cilium
4. Run `cilium install --version 1.16.4`
5. Wait for Cilium to be ready: `cilium status --wait`
6. Display Cilium status

**Cilium components deployed:**
- cilium DaemonSet (runs on all nodes)
- cilium-operator Deployment
- cilium-envoy DaemonSet

**Duration:** ~2-3 minutes for Cilium to be fully ready

---

### ğŸ”— STEP C: Workers Join & Verification

#### `step-c-workers-join.yml`
**Purpose:** Complete cluster setup  
**What it does:**

**Phase 1: Import playbooks/workers-join.yml**

**Phase 2: Wait for cluster readiness**
- `kubectl wait --for=condition=Ready node --all --timeout=10m`
- `kubectl rollout status ds/cilium -n kube-system --timeout=10m`

**Phase 3: Import playbooks/verify-cluster.yml**

**Phase 4: Final summary**
- Get cluster info
- Get all nodes (wide)
- Get all pods (all namespaces)
- Get Cilium status
- Display connection details

**Duration:** ~2-3 minutes

---

#### `playbooks/workers-join.yml`
**Purpose:** Join worker nodes to cluster  
**What it does:**
1. Retrieve join command from master (from file or regenerate)
2. Set join command as fact for all workers
3. Check if node already joined (check kubelet.conf)
4. Execute join command if not already joined
5. Ensure kubelet service is running

**Join command format:**
```bash
kubeadm join 192.168.100.x:6443 --token <TOKEN> \
    --discovery-token-ca-cert-hash sha256:<HASH>
```

---

#### `playbooks/verify-cluster.yml`
**Purpose:** Verify cluster health  
**What it does:**
1. Check all nodes are in Ready state
2. Count total nodes vs ready nodes
3. Verify all system pods are Running
4. Check Cilium status
5. Display cluster health summary

**Expected state:**
- Nodes: 3 total, 3 Ready
- Pods: 16 total Running
  - cilium: 3 (DaemonSet)
  - cilium-operator: 1
  - cilium-envoy: 3 (DaemonSet)
  - coredns: 2
  - kube-proxy: 3 (DaemonSet)
  - etcd: 1
  - kube-apiserver: 1
  - kube-controller-manager: 1
  - kube-scheduler: 1

---

### ğŸ› ï¸ Supporting Files

#### `playbooks/common-setup.yml`
**Purpose:** Reusable common setup tasks  
**Used by:** Various playbooks for common node configuration

#### `playbooks/reset-cluster.yml`
**Purpose:** Reset cluster to clean state  
**What it does:**
- Run `kubeadm reset` on all nodes
- Remove kubeconfig files
- Stop kubelet and containerd
- Clean CNI and Kubernetes directories

#### `cluster-manager.sh`
**Purpose:** Cluster lifecycle management  
**Commands:**
- `status` - Show cluster status
- `ssh-master` - SSH to master node
- `kubeconfig` - Download kubeconfig
- `destroy` - Destroy cluster
- `nodes` - List all nodes
- `pods` - List all pods

#### `scripts/setup-openstack-environment.sh`
**Purpose:** Initial OpenStack environment setup (ONE-TIME SETUP)

**When to use:**
- Fresh OpenStack AIO installation
- After restoring OpenStack from snapshot
- Need to recreate learning domain/project

**How to run:**
```bash
# Copy to OpenStack host and execute there
scp scripts/setup-openstack-environment.sh openstack:~/
ssh openstack
./setup-openstack-environment.sh
```

**What it does:**
1. **Domain & Project Setup**
   - Creates `learning` domain
   - Creates `learning-project` in learning domain
   - Creates `learning-admin` user (password: DomainAdmin123!)
   - Grants admin role to user

2. **Network Infrastructure**
   - Creates `public` external network (172.29.248.0/22)
   - Creates subnet with gateway 172.29.248.1
   - Sets allocation pool: 172.29.249.100 - 172.29.249.200
   - Enables DHCP

3. **Image Management**
   - Downloads Ubuntu 24.04 Noble cloud image
   - Uploads to Glance as `ubuntu-noble`
   - Sets image properties (hw_disk_bus=scsi, etc.)

4. **Flavor Creation**
   - Creates `my-large` flavor
   - Specs: 2 vCPU, 4GB RAM, 8GB disk
   - Optimized to prevent disk exhaustion on AIO

5. **Networking Fix (Critical for VM connectivity)**
   - Applies iptables NAT MASQUERADE rule
   - Rule: `-s 172.29.248.0/22 ! -d 172.29.248.0/22 -j MASQUERADE`
   - Installs `iptables-persistent` package
   - Saves rules for persistence across reboots

6. **OpenRC File Generation**
   - Creates `~/openrc-learning-admin` with credentials
   - Ready to source for cluster deployments

**Dependencies:**
- Requires `openstack` CLI installed on host (`/snap/bin/openstack`)
- Must run on OpenStack host (not local machine)
- Needs root/sudo access for iptables rules

**Run time:** ~5-10 minutes

**Output:**
- OpenStack learning environment ready
- Credentials file: `~/openrc-learning-admin`
- Network routing configured and persistent

**Note:** This script was updated to use direct `openstack` CLI instead of LXC utility container wrapper

#### `ansible.cfg`
**Purpose:** Ansible configuration  
**Settings:**
- host_key_checking = False
- retry_files_enabled = False
- inventory = inventory-dynamic.yml
- remote_user = ubuntu
- private_key_file = config/k8s-cluster-key.pem

---

## 4. EXECUTION SUMMARY

### Total Deployment Timeline

```
00:00 - Start deploy-complete.sh
00:01 - Step A: Terraform init/plan (30 seconds)
00:02 - Step A: Terraform apply (1 minute)
00:03 - Step A: Wait for VMs + SSH test (30 seconds)
00:03 - Step B starts
00:04 - Step B: Common setup on all nodes (2 minutes)
00:06 - Step B: System reboot (1 minute)
00:07 - Step B: Master init with kubeadm (2 minutes)
00:09 - Step B: Install Cilium CNI (2 minutes)
00:11 - Step C starts
00:11 - Step C: Workers join (30 seconds)
00:12 - Step C: Wait for cluster ready (2 minutes)
00:14 - Step C: Verification and summary (30 seconds)
00:15 - COMPLETE âœ…
```

**Total: ~15 minutes for complete 3-node cluster**

---

## 5. GIT REPOSITORY STRUCTURE

### Project Structure

```
k8s-automation/
â”œâ”€â”€ .gitignore                          
â”œâ”€â”€ README.md                           
â”œâ”€â”€ GIT-SETUP.md                        
â”œâ”€â”€ deploy-complete.sh                  
â”œâ”€â”€ cluster-manager.sh                  
â”œâ”€â”€ ansible.cfg                         
â”œâ”€â”€ step-a-provision-infrastructure.yml 
â”œâ”€â”€ step-b-kubernetes-init.yml          
â”œâ”€â”€ step-c-workers-join.yml             
â”œâ”€â”€ terraform/                          
â”‚   â”œâ”€â”€ provider.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ infrastructure.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ cloud-init.yaml
â”‚   â””â”€â”€ inventory.tpl
â”œâ”€â”€ playbooks/                          
â”‚   â”œâ”€â”€ common-setup.yml
â”‚   â”œâ”€â”€ master-init.yml
â”‚   â”œâ”€â”€ install-cilium.yml
â”‚   â”œâ”€â”€ workers-join.yml
â”‚   â”œâ”€â”€ verify-cluster.yml
â”‚   â””â”€â”€ reset-cluster.yml
â”œâ”€â”€ scripts/                            
â”‚   â”œâ”€â”€ setup-openstack-environment.sh
â”‚   â””â”€â”€ terraform-ops.sh
â””â”€â”€ config/                             
    â””â”€â”€ kubeadm-config.yaml.j2
```

---

## 6. QUICK REFERENCE

### One Command to Deploy Everything
```bash
./deploy-complete.sh
```

### Individual Steps (for debugging)
```bash
# Step A only
ansible-playbook step-a-provision-infrastructure.yml

# Step B only (requires Step A completed)
ansible-playbook -i inventory-dynamic.yml step-b-kubernetes-init.yml

# Step C only (requires Steps A & B completed)
ansible-playbook -i inventory-dynamic.yml step-c-workers-join.yml
```

### Terraform Operations
```bash
cd terraform
terraform init
terraform plan
terraform apply
terraform destroy
```

### Cluster Management
```bash
./cluster-manager.sh status      # Show cluster status
./cluster-manager.sh ssh-master  # SSH to master
./cluster-manager.sh kubeconfig  # Download kubeconfig
./cluster-manager.sh destroy     # Destroy cluster
```

---

**Documentation complete! Your automation is ready for production use. ğŸš€**
